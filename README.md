# Business Problem
## The problem for the lending organization is determining the creditworthiness of loan applicants. The main issue for the business is reducing the risk of default because it has a direct impact on the institution's sustainability and profitability. In order to reduce the possibility of defaults, accurate projections can help identify high-risk applicants and change loan terms accordingly.

# Project Obvjective
## The "Home Credit Default Risk" project aims to create prediction models that can precisely calculate the probability that customers would miss loan installments. The initiative seeks to offer insights to the lending institution for enhanced credit risk assessment and decision-making by using numerous data points and features.

# Group's Solution
## In order to address the objectives and issues outlined above, our team investigated the deployment of numerous distinct models. Logistic Regression, Random Forest, Naive Bayes, Support Vector, Gradient Boosting, and Extreme Gradient Boosting were some of the models used. All were carried out by diverse team members and had remarkably identical outcomes. It was simple to achieve high accuracy scores in the 90s due to the imbalance in our dataset, but it was obvious that these models - in their current state - were not performing as we had planned when looking at other statistics like AUC curves and recall.

# My Contribution
##   For my individiual work, I actively managed data preparation for this project, looked into missing values, and used EDA to comprehend feature-target correlations. I made sure that the data in the train set and test set were aligned by using efficient encoding techniques for categorical variables. I experimented with numerous machine learning methods during the modeling phase and assessed their effectiveness using accuracy ratings and cross-validation. My contributions include a methodical approach to data analysis, modeling, and decision-making, resulting in an all-inclusive solution for estimating the risk of home credit default. And for the group work, Naive Bayes and Support Vector Machine (SVM) were two candidate models I looked at during the modeling process. I used the train_test_split function to divide the data into training and testing sets in order to choose the best model. I applied the GaussianNB implementation to Naive Bayes and trained the model using training data. In order to assess the model's performance, I then made predictions based on the test data and determined the accuracy score. Similar to how I did with SVM, I enabled probability estimates and used the SVC implementation with a specific value for the C parameter. I used the training data to train the SVM model, the test data for prediction, and the accuracy score calculation. The most effective model would be the one with the highest accuracy score.

# Diffuclties that we faced
## The difficulties that we faced particularly internally in the group were miscommunication in the initial stages, but we figured and worked on them. In the project, we faced the problem of identifying the ideal accuracy of all the models which either sometimes was really low or really high which unnatural for a model. So by doing some more data cleaning and feature engineering, we figured out a way for these models that we were working on.

# What I learned from the project
## I gained knowledge of the full machine learning model construction process for predicting loan default risk through this assignment. I learned more about handling missing values and encoding categorical variables during the data preparation process. I also gained knowledge on how to spot and handle extreme values in the data. As the model's performance was enhanced, the process of feature engineering and introducing additional variables was exciting. I investigated numerous models and tried out hyperparameter tuning, including Logistic Regression, Random Forest, Gradient Boosting, Naive Bayes, and SVM. I was able to gauge the models' resilience through cross-validation evaluation. Overall, this research helped me better grasp how predictive models are created from beginning to end and how crucial data preparation is to getting correct results.
